{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"102mPfXqFZAXMaMbDg-MJDCgo_kIdGODl","authorship_tag":"ABX9TyNVe6N7C3zphk0R0w9U7E+l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HXgI9bOoFbnj"},"outputs":[],"source":["import numpy as np\n","class SeqEmbedding():\n","    def __init__(self):\n","        self.Dictionary = {}\n","\n","    def __Sequence_to_Numeric(self,k,sequence):\n","        if k==0:\n","            temp = [0] * self.size_of_vector\n","            temp[len(self.Dictionary)] = 1\n","            self.Dictionary[sequence] = temp\n","            return\n","        nucleotide = ['A','C','G','T']\n","        for n in nucleotide:\n","            self.__Sequence_to_Numeric(k-1,sequence+n)\n","        return\n","\n","    def fit(self, sequences, window_size, stride_size):\n","        self.size_of_vector = 4 ** window_size\n","        self.__Sequence_to_Numeric(window_size,\"\")\n","\n","        vectorized = []\n","\n","        for seq in sequences:\n","            first_layer_embedding = []\n","            for k in range(window_size, len(seq)+1, stride_size):\n","                try:\n","                    first_layer_embedding.append(self.Dictionary[seq[k-window_size:k]])\n","                except:\n","                    # exception may occur because of stride size, sometimes it may not get sequence of length window size, there will be a key not found exception in Dictionary\n","                    first_layer_embedding.append([0]*self.size_of_vector)\n","\n","            vector0 = []\n","            vector1 = []\n","            for i in range(len(first_layer_embedding)):\n","                if i>0:\n","                    vector1+=first_layer_embedding[i]\n","                vector0+=first_layer_embedding[i]\n","            vector1+=first_layer_embedding[0]\n","\n","            vectorized.append([vector0, vector1])\n","\n","        # Handling inequal length problem using zero padding\n","        max_len = 0\n","        for vec in vectorized:\n","            max_len = max(max_len, len(vec[0]))\n","        for i in range(len(vectorized)):\n","            required = max_len - len(vectorized[i][0])\n","\n","            vectorized[i][0]+=([0]*required)\n","            vectorized[i][0] = vectorized[i][0][:31752] # to fit into 252 * 252 pixel size, removing 120 elements\n","            vectorized[i][0] = np.array(vectorized[i][0])\n","\n","            vectorized[i][1]+=([0]*required)\n","            vectorized[i][1] = vectorized[i][1][:31752] # to fit into 252 * 252 pixel size, removing 120 elements\n","            vectorized[i][1] = np.array(vectorized[i][1])\n","\n","            vectorized[i] = np.array(vectorized[i])\n","\n","        return np.array(vectorized)\n","\n","\n","file = open(\"/content/drive/MyDrive/histone/H3.txt\",\"r\")\n","sequences = []\n","class_label = []\n","for line in file:\n","    if '>' in line:\n","        continue\n","    line = line[:-1]\n","    if len(line)>1:\n","        sequences.append(line)\n","    else:\n","        class_label.append(int(line))\n","file.close()\n","\n","from sklearn.model_selection import train_test_split\n","train_sz = (5000.0/len(sequences))\n","sequences, _sequences, class_label, _class_label = train_test_split(sequences, class_label, train_size = train_sz, random_state=42, stratify=class_label)\n","\n","class_label = np.array(class_label)\n","\n","from sklearn.model_selection import train_test_split\n","X_train_val, X_test, y_train_val, y_test = train_test_split(sequences, class_label, test_size=0.2, random_state=42, stratify=class_label)\n","\n","instance = SeqEmbedding()\n","X_train_val = instance.fit(sequences = X_train_val, window_size = 5, stride_size = 1)\n","X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1, random_state=42, stratify = y_train_val)\n","\n","X_train = X_train.reshape(X_train.shape[0],252, 252, 1)\n","X_val = X_val.reshape(X_val.shape[0], 252, 252, 1)\n","\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import keras\n","\n","model = tf.keras.models.Sequential([\n","                                    tf.keras.layers.Conv2D(16,(3,3),activation='relu',input_shape=(252,252,1)),\n","                                    tf.keras.layers.MaxPooling2D(2,2),\n","                                    tf.keras.layers.Conv2D(16,(3,3),activation='relu'),\n","                                    tf.keras.layers.MaxPooling2D(2,2),\n","                                    tf.keras.layers.Flatten(),\n","                                    tf.keras.layers.Dense(100,activation='relu'),\n","                                    tf.keras.layers.Dropout(0.5),\n","                                    tf.keras.layers.Dense(2,activation='softmax')\n","])\n","#model.summary()\n","\n","model.compile(optimizer='sgd',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n","\n","import matplotlib.pyplot as plt\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","es = EarlyStopping(monitor='val_accuracy',mode='max',verbose=1,patience=200)\n","mc = ModelCheckpoint('best_model.h5',monitor='val_accuracy',mode='max',verbose=1,save_best_only=True)\n","\n","hist = model.fit(X_train,y_train, validation_data=(X_val,y_val), epochs=300,batch_size=512,callbacks=[es,mc])\n","\n","plt.plot(hist.history['accuracy'],label='train')\n","plt.plot(hist.history['val_accuracy'],label='test')\n","plt.legend()\n","plt.show()\n","\n"]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","model = SVC(kernel = 'linear')\n","model.fit(X_train.reshape(X_train.shape[0], -1),y_train)\n","y_pred = model.predict(X_val.reshape(X_val.shape[0], -1))\n","print(f'Accuracy Score: {accuracy_score(y_val, y_pred)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"58GNECf9GySq","executionInfo":{"status":"ok","timestamp":1711093994127,"user_tz":-330,"elapsed":718572,"user":{"displayName":"Siddhant Jain","userId":"18398307266802481501"}},"outputId":"74aedce5-5d38-4318-d86a-40896e7890bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy Score: 0.7\n"]}]},{"cell_type":"code","source":["\n","# import pickle as pkl\n","model = tf.keras.models.Sequential([\n","                                    tf.keras.layers.Conv2D(16,(3,3),activation='relu',input_shape=(252,252,1)),\n","                                    tf.keras.layers.MaxPooling2D(2,2),\n","                                    tf.keras.layers.Conv2D(16,(3,3),activation='relu'),\n","                                    tf.keras.layers.MaxPooling2D(2,2),\n","                                    tf.keras.layers.Flatten(),\n","                                    tf.keras.layers.Dense(100,activation='relu'),\n","                                    tf.keras.layers.Dropout(0.5),\n","                                    tf.keras.layers.Dense(2,activation='softmax')\n","])\n","model.compile(optimizer='sgd',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n","model.load_weights('/content/best_model.h5')\n","\n","ins = SeqEmbedding()\n","X_test = ins.fit(sequences=X_test,window_size = 5, stride_size = 1)\n","\n","print(X_test.shape)\n","X_test = X_test.reshape(X_test.shape[0],252, 252, 1)\n","model.evaluate(X_test, y_test)\n","\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef\n","p = model.predict(X_test)\n","prediction = []\n","for x in p:\n","    if(x[0]>x[1]):\n","        prediction.append(0)\n","    else:\n","        prediction.append(1)\n","print(f'Accuracy: {accuracy_score(y_test,prediction)}')\n","print(f'Precision: {precision_score(y_test,prediction)}')\n","print(f'Recall: {recall_score(y_test,prediction)}')\n","print(f'F1 Score: {f1_score(y_test,prediction)}')\n","print(f'MCC Score: {matthews_corrcoef(y_test,prediction)}')"],"metadata":{"id":"LAUYPV9sO27w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9gDB0QGxIQ0o"},"execution_count":null,"outputs":[]}]}